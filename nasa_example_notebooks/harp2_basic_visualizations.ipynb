{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d44bdc-8131-4a19-a09e-9ec84247ff23",
   "metadata": {},
   "source": [
    "# Visualize Data from the Hyper-Angular Rainbow Polarimeter (HARP2)\n",
    "\n",
    "**Authors:** Sean Foley (NASA, MSU), Meng Gao (NASA, SSAI), Ian Carroll (NASA, UMBC)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "\n",
    "The following notebooks are **prerequisites** for this tutorial.\n",
    "\n",
    "- Learn with OCI: [Data Access][oci-data-access]\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "An [Earthdata Login][edl] account is required to access data from the NASA Earthdata system, including NASA ocean color data.\n",
    "\n",
    "</div>\n",
    "\n",
    "[edl]: https://urs.earthdata.nasa.gov/\n",
    "[oci-data-access]: https://oceancolor.gsfc.nasa.gov/resources/docs/tutorials/notebooks/oci_data_access/\n",
    "\n",
    "## Summary\n",
    "\n",
    "PACE has two Multi-Angle Polarimeters (MAPs): [SPEXone](https://pace.oceansciences.org/spexone.htm) and [HARP2](https://pace.oceansciences.org/harp2.htm). These sensors offer unique data, which is useful for its own scientific purposes and also complements the data from OCI. Working with data from the MAPs requires you to understand both multi-angle data and some basic concepts about polarization. This notebook will walk you through some basic understanding and visualizations of multi-angle polarimetry, so that you feel comfortable incorporating this data into your future projects.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook you will know:\n",
    "\n",
    "* How to acquire data from HARP2\n",
    "* How to plot geolocated imagery\n",
    "* Some basic concepts about polarization\n",
    "* How to make animations of multi-angle data\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "1. [Get Level-1C Data](#data)\n",
    "1. [Understanding Multi-Angle Data](#multiangle)\n",
    "1. [Understanding Polarimetry](#polarimetry)\n",
    "1. [Radiance to Reflectance](#reflectance)\n",
    "1. [Animating an Overpass](#animation)\n",
    "\n",
    "<a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38135c-633a-4546-a31e-8a108c5a5de5",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Begin by importing all of the packages used in this notebook. If your kernel uses an environment defined following the guidance on the [tutorials] page, then the imports will be successful.\n",
    "\n",
    "[tutorials]: https://oceancolor.gsfc.nasa.gov/resources/docs/tutorials/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from matplotlib import animation\n",
    "import cartopy.crs as ccrs\n",
    "import earthaccess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8015c45-c662-42b0-8c4a-de623b00c7d6",
   "metadata": {},
   "source": [
    "[back to top](#contents) <a name=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840f8f2-afca-4ac7-bd76-188b170cea97",
   "metadata": {},
   "source": [
    "## 2. Get Level-1C Data\n",
    "\n",
    "Download some HARP2 Level-1C data using the `short_name` value \"PACE_HARP2_L1C_SCI\" in `earthaccess.search_data`. Level-1C corresponds to geolocated imagery. This means the imagery coming from the satellite has been calibrated and assigned to locations on the Earth's surface. Note that this might take a while, depending on the speed of your internet connection, and the progress bar will seem frozen because we're only downloading one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ef8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tspan = (\"2024-05-20\", \"2024-05-20\")\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_HARP2_L1C_SCI\",\n",
    "    temporal=tspan,\n",
    "    count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = xr.open_dataset(paths[0])\n",
    "view = xr.open_dataset(paths[0], group=\"sensor_views_bands\").squeeze()\n",
    "geo = xr.open_dataset(paths[0], group=\"geolocation_data\").set_coords([\"longitude\", \"latitude\"])\n",
    "obs = xr.open_dataset(paths[0], group=\"observation_data\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af53e60",
   "metadata": {},
   "source": [
    "The `prod` dataset, as usual for OB.DAAC products, contains attributes but no variables. Merge it with the \"observation_data\" and \"geolocation_data\", setting latitude and longitude as auxiliary (e.e. non-index) coordinates, to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.merge((prod, obs, geo))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b6a00-adc5-4fed-80f9-f69cdc69ba93",
   "metadata": {},
   "source": [
    "[back to top](#contents) <a name=\"multiangle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d915a6-84fa-4160-829b-01d442113979",
   "metadata": {},
   "source": [
    "## 2. Understanding Multi-Angle Data\n",
    "\n",
    "HARP2 is a multi-spectral sensor, like OCI, with 4 spectral bands. These roughly correspond to green, red, near infrared (NIR), and blue (in that order). HARP2 is also multi-angle. These angles are with respect to the satellite track. Essentially, HARP2 is always looking ahead, looking behind, and everywhere in between. The number of angles varies per sensor. The red band has 60 angles, while the green, blue, and NIR bands each have 10.\n",
    "\n",
    "In the HARP2 data, the angles and the spectral bands are combined into one axis. I'll refer to this combined axis as HARP2's \"channels.\" Below, we'll make a quick plot both the viewing angles and the wavelengths of HARP2's channels. In both plots, the x-axis is simply the channel index.\n",
    "\n",
    "Pull out the view angles and wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = view[\"sensor_view_angle\"]\n",
    "wavelengths = view[\"intensity_wavelength\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b8267",
   "metadata": {},
   "source": [
    "Create a figure with 2 rows and 1 column and a reasonable size for many screens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_angle, ax_wavelength) = plt.subplots(2, 1, figsize=(14, 7))\n",
    "ax_angle.set_ylabel(\"View Angle (degrees)\")\n",
    "ax_angle.set_xlabel(\"Index\")\n",
    "ax_wavelength.set_ylabel(\"Wavelength (nm)\")\n",
    "ax_wavelength.set_xlabel(\"Index\")\n",
    "plot_data = [\n",
    "    (0, 10, \"green\", \"^\", \"green\"),\n",
    "    (10, 70, \"red\", \"*\", \"red\"),\n",
    "    (70, 80, \"black\", \"s\", \"NIR\"),\n",
    "    (80, 90, \"blue\", \"o\", \"blue\"),\n",
    "]\n",
    "for start_idx, end_idx, color, marker, label in plot_data:\n",
    "    ax_angle.plot(\n",
    "        np.arange(start_idx, end_idx),\n",
    "        angles[start_idx:end_idx],\n",
    "        color=color,\n",
    "        marker=marker,\n",
    "        label=label,\n",
    "    )\n",
    "    ax_wavelength.plot(\n",
    "        np.arange(start_idx, end_idx),\n",
    "        wavelengths[start_idx:end_idx],\n",
    "        color=color,\n",
    "        marker=marker,\n",
    "        label=label,\n",
    "    )\n",
    "ax_angle.legend()\n",
    "ax_wavelength.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b228a83-b172-4b7b-9447-94cc5b63ea80",
   "metadata": {},
   "source": [
    "[back to top](#contents) <a name=\"polarimetry\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39754f3-55bd-4687-a50d-6dcf4a4e9646",
   "metadata": {},
   "source": [
    "## 3. Understanding Polarimetry\n",
    "\n",
    "Both HARP2 and SPEXone conduct polarized measurements. Polarization describes the geometric orientation of the oscillation of light waves. Randomly polarized light (like light coming directly from the sun) has an approximately equal amount of waves in every orientation. When light reflects off certain surfaces or is scattered by small particles, it can become non-randomly polarized.\n",
    "\n",
    "Polarimetric data is typically represented using [Stokes vectors][stokes]. These have four components: I, Q, U, and V. Both HARP2 and SPEXone are only sensitive to linear polarization, and do not detect circular polarization. Since the V component corresponds to circular polarization, the data only includes the I, Q, and U elements of the Stokes vector.\n",
    "\n",
    "The I, Q, and U components of the Stokes vector are separate variables in the `obs` dataset.\n",
    "\n",
    "[stokes]: https://en.wikipedia.org/wiki/Stokes_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea90c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stokes = dataset[[\"i\", \"q\", \"u\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6299e",
   "metadata": {},
   "source": [
    "Let's make a plot of the I, Q, and U components of our Stokes vector, using the RGB channels, which will help our eyes make sense of the data. We'll use the view that is closest to pointing straight down, which is called the \"nadir\" view. It is important to understand that, because HARP2 is a pushbroom sensor with a wide swath, the sensor zenith angle at the edges of the swath will still be high. It's only a true nadir view close to the center of the swath. Still, the average sensor zenith angle will be lowest in this view.)\n",
    "\n",
    "The first 10 channels are green, the next 60 channels are red, and the final 10 channels are blue (we're skipping NIR).\n",
    "In each of those groups of channels, we get the index of the minimum absolute value of the camera angle, corresponding to our nadir view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6baa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_nadir_idx = np.argmin(np.abs(angles[:10].values))\n",
    "red_nadir_idx = 10 + np.argmin(np.abs(angles[10:70].values))\n",
    "blue_nadir_idx = 80 + np.argmin(np.abs(angles[80:].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466a75a",
   "metadata": {},
   "source": [
    "Then, get the data at the nadir indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stokes = stokes.isel(\n",
    "    {\n",
    "        \"number_of_views\": [red_nadir_idx, green_nadir_idx, blue_nadir_idx],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66be8b",
   "metadata": {},
   "source": [
    "A few adjustments make the image easier to visualize. First, normalize the data between 0 and 1. Second, bring out some of the darker colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stokes = (rgb_stokes - rgb_stokes.min()) / (rgb_stokes.max() - rgb_stokes.min())\n",
    "rgb_stokes = rgb_stokes ** (3 / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc83e33",
   "metadata": {},
   "source": [
    "Since the nadir view is not processed at swath edges, a better image will result from finding a valid window within the dataset. Using just the array for the I component, we crop the `rgb_stokes` dataset using the `where` attribute and some boolean logic applied across different dimensions of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07884e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = rgb_stokes[\"i\"].notnull().all(\"number_of_views\")\n",
    "crop_rgb_stokes = rgb_stokes.where(\n",
    "    window.any(\"bins_along_track\") & window.any(\"bins_across_track\"),\n",
    "    drop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f475e7b",
   "metadata": {},
   "source": [
    "The granule crosses the 180 degree longitude, so we set up the figure and subplots to use a Plate Carree projection shifted to center on a -170 longitude. The data has coordinates from the default (i.e. centered at 0 longitude) Plate Carree projection, so we give that CRS as a `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_proj = ccrs.PlateCarree(-170)\n",
    "crs_data = ccrs.PlateCarree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0247b8e-5346-4f4b-ac70-99d18db3b9b6",
   "metadata": {},
   "source": [
    "The figure will hav 1 row and 3 columns, for each of the I, Q, and U arrays, spanning a width suitable for many screens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c364692-f597-4dc6-a590-447f75058def",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5), subplot_kw={\"projection\": crs_proj})\n",
    "fig.suptitle(f'{prod.attrs[\"product_name\"]} RGB')\n",
    "\n",
    "for i, (key, value) in enumerate(crop_rgb_stokes.items()):\n",
    "    ax[i].pcolormesh(value[\"longitude\"], value[\"latitude\"], value, transform=crs_data)\n",
    "    ax[i].gridlines(draw_labels={\"bottom\": \"x\", \"left\": \"y\"}, linestyle=\"--\")\n",
    "    ax[i].coastlines(color=\"grey\")\n",
    "    ax[i].set_title(key.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861556f",
   "metadata": {},
   "source": [
    "It's pretty plain to see that the I plot makes sense to the eye: we can see clouds over the Pacific Ocean (this scene is south of the Cook Islands and east of Australia). This is because the I component of the Stokes vector corresponds to the total intensity. In other words, this is roughly what your eyes would see. However, the Q and U plots don't quite make as much sense to the eye. We can see that there is some sort of transition in the middle, which is the satellite track. This transition occurs in both plots, but is stronger in Q. This gives us a hint: the type of linear polarization we see in the scene depends on the angle with which we view the scene.\n",
    "\n",
    "[This Wikipedia plot](https://upload.wikimedia.org/wikipedia/commons/3/31/StokesParameters.png) is very helpful for understanding what exactly the Q and U components of the Stokes vector mean. Q describes how much the light is oriented in -90°/90° vs. 0°/180°, while U describes how much light is oriented in -135°/45°; vs. -45°/135°.\n",
    "\n",
    "Next, let's take a look at the degree of linear polarization (DoLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_dolp = dataset[\"dolp\"].isel(\n",
    "    {\n",
    "        \"number_of_views\": [red_nadir_idx, green_nadir_idx, blue_nadir_idx],\n",
    "    }\n",
    ")\n",
    "crop_rgb_dolp = rgb_dolp.where(\n",
    "    window.any(\"bins_along_track\") & window.any(\"bins_across_track\"),\n",
    "    drop=True,\n",
    ")\n",
    "crop_rgb = xr.merge((crop_rgb_dolp, crop_rgb_stokes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84281c9",
   "metadata": {},
   "source": [
    "Create a figure with 1 row and 2 columns, having a good width for many screens, that will use the projection defined above. For the two columns, we iterate over just the I and DoLP arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8), subplot_kw={\"projection\": crs_proj})\n",
    "fig.suptitle(f'{prod.attrs[\"product_name\"]} RGB')\n",
    "\n",
    "for i, (key, value) in enumerate(crop_rgb[[\"i\", \"dolp\"]].items()):\n",
    "    ax[i].pcolormesh(value[\"longitude\"], value[\"latitude\"], value, transform=crs_data)\n",
    "    ax[i].gridlines(draw_labels={\"bottom\": \"x\", \"left\": \"y\"}, linestyle=\"--\")\n",
    "    ax[i].coastlines(color=\"grey\")\n",
    "    ax[i].set_title(key.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d498cb4",
   "metadata": {},
   "source": [
    "For a different perspective on DoLP, line plots of the channels averaged over the two spatial dimensions show the clear minimum associated with the nadir view angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51672e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dolp_mean = dataset[\"dolp\"].mean([\"bins_along_track\", \"bins_across_track\"])\n",
    "dolp_mean = (dolp_mean - dolp_mean.min()) / (dolp_mean.max() - dolp_mean.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "wv_uq = np.unique(wavelengths.values)\n",
    "plot_data = [(\"b\", \"o\"), (\"g\", \"^\"), (\"r\", \"*\"), (\"k\", \"s\")]\n",
    "for wv_idx in range(4):\n",
    "    wv = wv_uq[wv_idx]\n",
    "    wv_mask = wavelengths.values == wv\n",
    "    c, m = plot_data[wv_idx]\n",
    "    ax.plot(\n",
    "        angles.values[wv_mask],\n",
    "        dolp_mean[wv_mask],\n",
    "        color=c,\n",
    "        marker=m,\n",
    "        markersize=7,\n",
    "        label=str(wv),\n",
    "    )\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Nominal View Angle (°)\")\n",
    "ax.set_ylabel(\"DoLP\")\n",
    "ax.set_title(\"Mean DoLP by View Angle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fd6f6-cec0-4882-aa0e-a192c5c96316",
   "metadata": {},
   "source": [
    "[back to top](#contents) <a name='reflectance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741500f4-0b0f-40e6-ab1b-568a4f67232c",
   "metadata": {},
   "source": [
    "## 4. Radiance to Reflectance\n",
    "\n",
    "We can convert radiance into reflectance. For a more in-depth explanation, see [here](https://seadas.gsfc.nasa.gov/help-9.0.0/rad2refl/Rad2ReflAlgorithmSpecification.html#:~:text=Radiance%20is%20the%20variable%20directly,it%2C%20and%20it%20is%20dimensionless). This conversion compensates for the differences in appearance due to the viewing angle and sun angle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c963c1-267f-4741-8026-d9e5edc2b424",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The radiances collected by HARP2 often need to be converted, using additional properties, to reflectances. Write the conversion as a function, because you may need to repeat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rad_to_refl(rad, f0, sza, r):\n",
    "    \"\"\"Convert radiance to reflectance.\n",
    "    \n",
    "    Args:\n",
    "        rad: Radiance.\n",
    "        f0: Solar irradiance.\n",
    "        sza: Solar zenith angle.\n",
    "        r: Sun-Earth distance (in AU).\n",
    "\n",
    "    Returns: Reflectance.\n",
    "    \"\"\"\n",
    "    return (r**2) * np.pi * rad / np.cos(sza * np.pi / 180) / f0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57041d8a-0509-498c-b631-92cfa4b3a4f0",
   "metadata": {},
   "source": [
    "The difference in appearance (after matplotlib automatically normalizes the data) is negligible, but the difference in the physical meaning of the array values is quite important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "refl = rad_to_refl(\n",
    "    rad=dataset[\"i\"],\n",
    "    f0=view[\"intensity_f0\"],\n",
    "    sza=dataset[\"solar_zenith_angle\"],\n",
    "    r=float(dataset.attrs[\"sun_earth_distance\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e21257",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].imshow(dataset[\"i\"].sel({\"number_of_views\": red_nadir_idx}), cmap=\"gray\")\n",
    "ax[0].set_title(\"Radiance\")\n",
    "ax[1].imshow(refl.sel({\"number_of_views\": red_nadir_idx}), cmap=\"gray\")\n",
    "ax[1].set_title(\"Reflectance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63290a0e",
   "metadata": {},
   "source": [
    "Create a line plot of the mean reflectance for each view angle and spectral channel. The flatness of this plot serves as a sanity check that nothing has gone horribly wrong with our data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "wv_uq = np.unique(wavelengths.values)\n",
    "plot_data = [(\"b\", \"o\"), (\"g\", \"^\"), (\"r\", \"*\"), (\"black\", \"s\")]\n",
    "refl_mean = refl.mean([\"bins_along_track\", \"bins_across_track\"])\n",
    "for wv_idx in range(4):\n",
    "    wv = wv_uq[wv_idx]\n",
    "    wv_mask = wavelengths.values == wv\n",
    "    c, m = plot_data[wv_idx]\n",
    "    ax.plot(\n",
    "        angles.values[wv_mask],\n",
    "        refl_mean[wv_mask],\n",
    "        color=c,\n",
    "        marker=m,\n",
    "        markersize=7,\n",
    "        label=str(wv),\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Nominal View Angle (°)\")\n",
    "ax.set_ylabel(\"Reflectance\")\n",
    "ax.set_title(\"Mean Reflectance by View Angle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cd0fc-d328-476e-b445-375eed6e97c3",
   "metadata": {},
   "source": [
    "[back to top](#contents) <a name='animation'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c0022-2282-40d8-b4e5-a51f54a745d1",
   "metadata": {},
   "source": [
    "## 5. A Simple Animation\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "\n",
    "WARNING: there is some flickering in the animation displayed in this section.\n",
    "\n",
    "</div>\n",
    "\n",
    "All that is great for looking at a single angle at a time, but it doesn't capture the multi-angle nature of the instrument. Multi-angle data innately captures information about 3D structure. To get a sense of that, we'll make an animation of the scene with the 60 viewing angles available for the red band.\n",
    "\n",
    "We are going to generate this animation without using the latitude and longitude coordinates. If you use XArray's `plot` as above with coordinates, you could use a projection. However, that can be a little slow for all animation \"frames\" available with HARP2. This means there will be some stripes of what seems like missing data at certain angles. These stripes actually result from the gridding of the multi-angle data, and are not a bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574940b5",
   "metadata": {},
   "source": [
    "Get the reflectances of just the red channel, and normalize the reflectance to lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "refl_red = refl[..., 10:70]\n",
    "refl_pretty = (refl_red - refl_red.min()) / (refl_red.max() - refl_red.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a87e4",
   "metadata": {},
   "source": [
    "A very mild Gaussian filter over the angular axis will improve the animation's smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "refl_pretty.data = gaussian_filter1d(refl_pretty, sigma=0.5, truncate=2, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a057035",
   "metadata": {},
   "source": [
    "Raising the image to the power 2/3 will brighten it a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refl_pretty = refl_pretty ** (2 / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a1b4e",
   "metadata": {},
   "source": [
    "Append all but the first and last frame in reverse order, to get a 'bounce' effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5668b-f646-474a-a219-b673187ea795",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.arange(refl_pretty.sizes[\"number_of_views\"])\n",
    "frames = np.concatenate((frames, frames[-1::-1]))\n",
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959f3d2",
   "metadata": {},
   "source": [
    "In order to display an animation in a Jupyter notebook, the \"backend\" for matplotlib has to be explicitly set to \"widget\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22993d5-6109-4a54-9c5b-a69d37a1fe61",
   "metadata": {},
   "source": [
    "Now we can use `matplotlib.animation` to create an initial plot, define a function to update that plot for each new frame, and show the resulting animation. When we create the inital plot, we get back the object called `im` below. This object is an instance of `matplotlib.artist.Artist` and is responsible for rendering data on the axes. Our `update` function uses that artist's `set_data` method to leave everything in the plot the same other than the data used to make the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(refl_pretty[{\"number_of_views\": 0}], cmap=\"gray\")\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    im.set_data(refl_pretty[{\"number_of_views\": i}])\n",
    "    return im\n",
    "\n",
    "an = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=30)\n",
    "filename = f'harp2_red_anim_{dataset.attrs[\"product_name\"].split(\".\")[1]}.gif'\n",
    "an.save(filename, writer=\"pillow\")\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93d0fa58-af8a-44e8-b1ee-dce59c49180f",
   "metadata": {},
   "source": [
    "This scene is a great example of multi-layer clouds. You can use the parallax effect to distinguish between these layers.\n",
    "\n",
    "The [sunglint](https://en.wikipedia.org/wiki/Sunglint) is an obvious feature, but you can also make out the [opposition effect](https://en.wikipedia.org/wiki/Opposition_surge) on some of the clouds in the scene. These details would be far harder to identify without multiple angles!\n",
    "\n",
    "![A multi-angle HARP2 animation](harp2_red_anim_20240519T235950.gif)\n",
    "\n",
    "Notice the cell ends with `plt.close()` rather than the usual `plt.show()`. By default, `matplotlib` will not display an animation. To view the animation, we saved it as a file and displayed the result in the next cell. Alternatively, you could change the default by executing `%matplotlib widget`. The `widget` setting, which works in Jupyter Lab but not on a static website, you can use `plt.show()` as well as `an.pause()` and `an.resume()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bb8b3",
   "metadata": {},
   "source": [
    "[back to top](#contents)\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "You have completed the notebook giving a first look at HARP2 data. More notebooks are comming soon!\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
